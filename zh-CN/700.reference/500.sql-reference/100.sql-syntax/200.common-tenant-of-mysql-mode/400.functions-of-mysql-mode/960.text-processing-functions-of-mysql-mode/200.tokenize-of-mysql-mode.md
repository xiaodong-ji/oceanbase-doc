| Description   |                 |
|---------------|-----------------|
| keywords      |                 |
| dir-name      |                 |
| dir-name-en   |                 |
| tenant-type   | MySQL Mode      |

# TOKENIZE

## 描述

该函数用于将文本按照指定的分词器及 Json 形式的参数，输出分词结果。

## 语法

```sql
TOKENIZE('text', ['parser'], ['behavior_ctrl'])
```

## 参数解释

|    **字段**   | **说明** |
|---------------|----------|
| text          | 表示文本。支持 `TEXT`、`CHAR` 和 `VARCHAR` 类型的数据。|
| parser        | 表示分词器名称。支持 `BENG`（基础英文）、`NGRAM`（中文） 和 `SPACE`（空格）等分词器。|
| behavior_ctrl | JSON 形式的参数指定，可选配置项如下：<ul><li>`stopwords`：停词表，如果无，按全局默认配置，应用全局停词；如果为空，则为空。</li><li>`case`：如果无，则按系统默认行为处理；如果有，按 upper/lower 进行处理。</li><li>`output`：有以下格式：<ul><li>`default`：输出仅包含词元的 `json array, ["hello", "world", "english"]`。</li><li>`all`：包含 `doc_len` 和词频的 `json object`，如 `{"doc_len":3, "tokens":["i":1, "love":1, "china":1]}`。</li><li>`additional-args`：["token_size:2"]} 特定分词器的参数，如`ngram`。</li></ul>|

## 示例

使用 `TOKENIZE` 函数来将字符串 `I Love China` 分解成单词，并使用 `beng` 作为分隔符。然后使用 `JSON` 格式的参数来设置输出选项。

```sql
SELECT TOKENIZE('I Love China','beng', '[{"output": "all"}]');
```

返回结果如下：

```shell
+--------------------------------------------------------+
| TOKENIZE('I Love China','beng', '[{"output": "all"}]') |
+--------------------------------------------------------+
| {"tokens": [{"love": 1}, {"china": 1}], "doc_len": 2}  |
+--------------------------------------------------------+
1 row in set
```
